from dessn.framework.parameter import Parameter, ParameterObserved, ParameterLatent, \
    ParameterUnderlying, ParameterTransformation, ParameterDiscrete
from dessn.framework.edge import EdgeTransformation
from dessn.utility.hdemcee import EmceeWrapper
import numpy as np
import logging
import emcee
from emcee.utils import MPIPool
import sys
from scipy.optimize import fmin_bfgs
import types
import copyreg
import itertools
import scipy.optimize
from scipy.misc import logsumexp


class Model(object):
    """ A generalised framework for use in arbitrary situations.

    A framework is, at heart, simply a collection of nodes and edges. Apart from simply
    being a container in which to place nodes and edges, the framework is also responsible
    for figuring out how to connect edges (which map to parameters) with the right nodes,
    for sorting edges such that when an edge is evaluated all its required data has been
    generated by other nodes or edges, for managing the ``emcee`` running, and also for
    generating the visual PGMs.


    Parameters
    ----------
    model_name : str
        The framework name, used for serialisation
    """

    def __init__(self, model_name):
        self.model_name = model_name
        self.logger = logging.getLogger(__name__)
        self.nodes = []
        self.edges = []
        self._node_dict = {}
        self._node_indexes = {}
        self._observed_nodes = []
        self._latent_nodes = []
        self._transformation_nodes = []
        self._underlying_nodes = []
        self._discrete_nodes = []
        self._discrete_params = []
        self._in = {}
        self._out = {}
        self._theta_names = []
        self._theta_labels = []
        self._ordered_edges = []
        self._num_actual = None
        self.data = []
        self._finalised = False
        self.flat_chain = None
        self.num_temps = None
        self._labels = []
        self.master = True
        self._node_groups = {}
        self.n = None
        self.epsilon = np.sqrt(np.finfo(float).eps)

    def add_node(self, node):
        """ Adds a node into the models collection of nodes.

        Parameters
        ----------
        node : :class:`.Parameter`
        """
        assert isinstance(node, Parameter), \
            "Supplied parameter is not a recognised Parameter object"
        assert node.name not in self._node_dict.keys(), \
            "Parameter %s is already in the framework" % node.name
        assert node.label not in self._labels, \
            "Label %s is already in the framework" % node.label
        self._labels.append(node.label)
        self.nodes.append(node)
        self._node_dict[node.name] = node
        self._in[node.name] = []
        self._out[node.name] = []
        if isinstance(node, ParameterObserved):
            self._observed_nodes.append(node)
        elif isinstance(node, ParameterDiscrete):
            self._discrete_nodes.append(node)
            self._discrete_params.append(node.name)
        elif isinstance(node, ParameterLatent):
            self._latent_nodes.append(node)
        elif isinstance(node, ParameterTransformation):
            self._transformation_nodes.append(node)
        elif isinstance(node, ParameterUnderlying):
            self._underlying_nodes.append(node)

        self._finalised = False

    def add_edge(self, edge):
        """ Adds an edge into the models collection of edges

        Parameters
        ----------
        edge : :class:`.Edge`
        """
        self.edges.append(edge)
        for p in edge.probability_of:
            for g in edge.given:
                self._in[g].append(p)
                self._out[p].append(g)
        self._finalised = False

    def _validate_model(self):
        assert len(self._underlying_nodes) > 0, "No underlying parameters to constrain"
        assert len(self._observed_nodes) > 0, "No observed nodes found"
        for node in self.nodes:
            name = node.name
            if isinstance(node, ParameterObserved):
                assert len(self._in[name]) + len(self._out[name]) > 0, \
                    "Unconnected node %s" % name
            elif isinstance(node, ParameterLatent) or isinstance(node, ParameterTransformation):
                assert len(self._in[name]) + len(self._out[name]) > 0, \
                    "Unconnected node %s" % name
            elif isinstance(node, ParameterUnderlying):
                assert len(self._in[name]) > 0, \
                    "Underlying parameter %s has no incoming edges" % name
                assert len(self._out[name]) == 0, \
                    "Underlying parameter %s should not have an outgoing edge" % name

    def _create_data_structures(self):
        self.data = {node.name: node.data for node in self._observed_nodes}
        for key in self.data:
            if self.n is None:
                self.n = len(self.data[key])
            else:
                assert self.n == len(self.data[key]), \
                    "Data sizes must agree. Have %d vs %d" % (self.n, len(self.data[key]))

        for node in self._underlying_nodes:
            self._theta_names.append(node.name)
            self._theta_labels.append(node.label)
        self._num_actual = len(self._theta_names)
        for node in self._latent_nodes:
            self._theta_names += [node.name] * node.get_num_latent()
        num_edges = len(self.edges)
        observed_names = [node.name for node in self.nodes
                          if not isinstance(node, ParameterTransformation)]
        self._ordered_edges = []
        count = 0
        max_count = 100
        while len(self._ordered_edges) < num_edges:
            for edge in self.edges:
                if edge in self._ordered_edges:
                    continue
                if isinstance(edge, EdgeTransformation):
                    requirements = edge.given
                else:
                    requirements = edge.given + edge.probability_of
                unsatisfied_requirements = [r for r in requirements if r not in observed_names]
                if len(unsatisfied_requirements) == 0:
                    self._ordered_edges.append(edge)
                    if isinstance(edge, EdgeTransformation):
                        for val in edge.probability_of:
                            observed_names.append(val)
            count += 1
            assert count < max_count, \
                "Model edges cannot be ordered. Please double check your edges"

    def finalise(self):
        """ Finalises the framework.

        This method runs consistency checks on the framework (making sure there are not orphaned
        nodes, edges to parameters that do not exist, etc), and in doing so links the right
        edges to the right nodes and determines the order in which edges should be evaluated.

        You can manually call this method after setting all nodes and edges to confirm as
        early as possible that the framework is valid. If you do not call it manually,
        this method is invoked by the framework when requesting concrete information,
        such as the PGM or framework fits.
        """
        self._validate_model()
        self._create_data_structures()
        self._finalised = True
        self.logger.info("Model validation passed")

    def _get_theta_dict(self, theta):
        result = {}
        arrs = {}
        data = self.data.copy()
        make_array = []
        for theta, theta_name in zip(theta, self._theta_names):
            if self._theta_names.count(theta_name) == 1:
                result[theta_name] = theta
            else:
                if theta_name not in arrs:
                    arrs[theta_name] = []
                    make_array.append(theta_name)
                arrs[theta_name].append(theta)
        for m in make_array:
            data[m] = np.array(arrs[m])
        return result, data

    def get_log_prior(self, theta):
        theta_dict, data = self._get_theta_dict(theta)
        return self._get_log_prior(theta_dict)

    def get_log_likelihood(self, theta):
        theta_dict, data = self._get_theta_dict(theta)
        return self._get_log_likelihood(theta_dict, data)

    def _get_log_likelihood(self, theta_dict, data):
        theta_dict.update(data)
        probability = self._get_edge_likelihood(theta_dict, self._ordered_edges[:])
        return probability.sum()

    def get_log_posterior(self, theta):
        theta_dict, data = self._get_theta_dict(theta)
        probability = self._get_log_prior(theta_dict)
        if np.isfinite(probability):
            probability += self._get_log_likelihood(theta_dict, data)
        return probability

    def _get_dependencies(self, edges, dependency_name):
        dep_edge = []
        for edge in edges:
            if isinstance(edge, EdgeTransformation) and dependency_name in edge.given:
                for name in edge.probability_of:
                    dep_edge += self._get_dependencies(edges, name)
            elif dependency_name in edge.given + edge.probability_of:
                dep_edge.append(edge)
        # Make list unique and order according to ordered_edges
        dep_edge = list(set(dep_edge))
        ordered = [a for a in self._ordered_edges if a in dep_edge]
        return ordered

    def _get_edge_likelihood(self, theta_dict, edges):
        edge_index = 0
        probability = None
        while edge_index < len(edges):
            edge = edges[edge_index]
            dependencies = edge.given + edge.probability_of
            discretes = [parameter for parameter in dependencies
                         if isinstance(self._node_dict[parameter], ParameterDiscrete)]
            unfilled = [d for d in discretes if d not in theta_dict]
            if len(unfilled) > 0:
                first_name = unfilled[0]
                first_node = self._node_dict[first_name]
                unfilled_dependencies = first_node.get_discrete_requirements()
                to_pass = {k: theta_dict[k] for k in unfilled_dependencies}

                discrete = first_node.get_discrete(to_pass)
                dependent_edges = self._get_dependencies(edges, first_name)
                edges = [e for e in edges if e not in dependent_edges]
                t = theta_dict.copy()
                if type(discrete) == tuple:
                    n = 0
                    for key in t:
                        value = t[key]
                        if type(value) == list:
                            n = len(value)
                            t[key] = value * len(discrete)
                        elif type(value) == np.ndarray:
                            n = len(value)
                            t[key] = np.tile(value, len(discrete))
                    assert n > 0, "No observational data found to effect!"
                    t[first_name] = np.repeat(discrete, self.n)
                    combine = np.tile(np.arange(self.n), len(discrete))
                    result = self._get_edge_likelihood(t, dependent_edges)
                    indexes = np.unique(combine)
                    for i in indexes:
                        result[np.argmax(combine == i)] = logsumexp(result[combine == i])
                elif type(discrete) == list:
                    combine = np.zeros(len(discrete))
                    n = len(discrete)
                    d2 = discrete[:]
                    c = 1
                    for i, element in enumerate(discrete):
                        if type(element) in [list, tuple]:
                            combine[i] = c
                            d2[i] = element[0]
                            end_elements = element[1:]
                            d2 = d2 + end_elements
                            combine = np.concatenate((combine, c * np.ones(len(end_elements))))
                            for key in t:
                                value = t[key]
                                if type(value) == list:
                                    arr = [value[i]] * len(end_elements)
                                    t[key] = value + arr
                                elif type(value) == np.ndarray:
                                    arr = [value[i]] * len(end_elements)
                                    t[key] = np.concatenate((value, arr))
                            c += 1
                    t[first_name] = d2
                    result = self._get_edge_likelihood(t, dependent_edges)
                    indexes = np.unique(combine)

                    for i in indexes:
                        if i == 0:
                            continue
                        result[np.argmax(combine == i)] = logsumexp(result[combine == i])
                else:
                    raise ValueError("Discrete result is not a tuple or a list! %s" % discrete)
                # if type(result) != np.ndarray:
                #     result = np.array(result)
                assert len(result.shape) == 1
                if probability is None:
                    probability = result[:n]
                else:
                    probability += result[:n]
            else:
                edge_index += 1
                if isinstance(edge, EdgeTransformation):
                    theta_dict.update(self._get_transformation(theta_dict, edge))
                else:
                    result = self._get_log_likelihood_edge(theta_dict, edge)
                    if type(result) != np.ndarray:
                        result = np.array(result)
                    assert len(result.shape) == 1
                    if probability is None:
                        probability = result
                    else:
                        probability += result
        return probability

    def _get_log_posterior_grad(self, theta):
        grad = scipy.optimize.approx_fprime(theta, self.get_log_posterior, self.epsilon)
        return grad

    def _get_negative_log_posterior(self, theta):
        val = self.get_log_posterior(theta)
        return -val

    def _get_suggestion(self):
        node_sorted = []
        for name in self._theta_names:
            node = self._node_dict[name]
            if node not in node_sorted:
                node_sorted.append(node)
        theta = []
        data = self.data
        for node in node_sorted:
            reqs = node.get_suggestion_requirements()
            node_data = {key: data[key] for key in reqs}
            suggestion = node.get_suggestion(node_data)
            if type(suggestion) == np.ndarray:
                theta += suggestion.tolist()
            elif type(suggestion) == list:
                theta += suggestion
            else:
                theta.append(suggestion)
        return theta

    def _get_suggestion_sigma(self):
        node_sorted = []
        for name in self._theta_names:
            node = self._node_dict[name]
            if node not in node_sorted:
                node_sorted.append(node)
        sigmas = []
        data = self.data
        for node in node_sorted:
            reqs = node.get_suggestion_requirements()
            node_data = {key: data[key] for key in reqs}
            suggestion = node.get_suggestion_sigma(node_data)
            if type(suggestion) == np.ndarray:
                sigmas += suggestion.tolist()
            elif type(suggestion) == list:
                sigmas += suggestion
            else:
                sigmas.append(suggestion)
        return sigmas

    def _get_starting_position(self, num_walkers):
        num_dim = len(self._theta_names)
        self.logger.debug("Generating starting guesses")
        p0 = self._get_suggestion()
        sigmas = self._get_suggestion_sigma()
        self.logger.debug("Initial position is:  %s" % p0)
        if len(p0) < 20:
            # TODO: confirm. Removing this for high dimensions, as it seems to be ineffective
            optimised = fmin_bfgs(self._get_negative_log_posterior, p0, disp=0)
            self.logger.debug("Optimised position is: %s" % optimised)
        else:
            optimised = p0

        std = np.random.uniform(low=-1, high=1, size=(num_walkers, num_dim)) * \
              np.array(sigmas).reshape((1, -1))
        start = optimised + std
        return start

    def _get_log_prior(self, theta_dict):
        result = []
        for node in self._underlying_nodes:
            p = node.get_log_prior({node.name: theta_dict[node.name]})
            if np.isnan(p):
                self.logger.error("Got NaN probability from %s: %s" % (node, theta_dict))
                raise ValueError("NaN")

            result.append(p)
        return np.sum(result)

    def _get_transformation(self, theta_dict, edge):
        result = edge.get_transformation(dict((key, theta_dict[key]) for key in edge.given))
        return result

    def _get_log_likelihood_edge(self, theta_dict, edge):
        result = edge.get_log_likelihood(dict((key, theta_dict[key])
                                              for key in edge.given + edge.probability_of))
        if np.any(np.isnan(result)):
            self.logger.error("Got NaN probability from %s: %s" % (edge, theta_dict))
            raise ValueError("NaN")
        return result

    def get_pgm(self, filename=None):  # pragma: no cover
        """ Renders (and returns) a PGM of the current framework.

        Parameters
        ----------
        filename : str, optional
            if the filename is set, the PGM is saved to file in the top level ``plots`` directory.

        Returns
        -------
        :class:`daft.PGM`
            The ``daft`` PGM class, for further customisation if required.
        """
        from dessn.utility.newtonian import NewtonianPosition
        from matplotlib import rc
        import daft
        if not self._finalised:
            self.finalise()

        self.logger.info("Generating PGM")
        rc("font", family="serif", size=8)
        rc("text", usetex=True)

        x_size = 9
        y_size = 8
        border = 1
        node_name_dict = {}
        reverse_dict = {}
        n = []
        e = []
        t = []
        b = []

        for node in self.nodes:
            value = node.name if node.group is None else node.group
            if value not in n:
                n.append(value)
            if node in self._observed_nodes:
                b.append(value)
            if node in self._underlying_nodes:
                t.append(value)
            node_name_dict[node.name] = value
            if value not in reverse_dict:
                reverse_dict[value] = {"names": [], "labels": []}
            reverse_dict[value]["node"] = node
            reverse_dict[value]["labels"].append(node.label)

        for edge in self.edges:
            for g in edge.given:
                for p in edge.probability_of:
                    e.append([node_name_dict[g], node_name_dict[p]])

        self.logger.debug("Using Newtonian positioner to position %d nodes and %d edges" %
                          (len(n), len(e)))

        positioner = NewtonianPosition(n, e, top=t, bottom=b)
        x, y = positioner.fit()
        x = (x_size - 2 * border) * x + border
        y = (y_size - 2 * border) * y + border

        self.logger.debug("Creating PGM from positioner results")
        pgm = daft.PGM([x_size, y_size], origin=[0., 0.2], observed_style='inner')
        for value, x, y in zip(n, x, y):
            node = reverse_dict[value]["node"]
            obs = node in self._observed_nodes
            fixed = node in self._transformation_nodes
            node_name = value
            if node.group is not None:
                node_label = node.group.replace(" ", "\n") + "\n"
            else:
                node_label = ""
            node_label += ", ".join(reverse_dict[value]["labels"])
            pgm.add_node(daft.Node(node_name, node_label, x, y, scale=1.6, aspect=1.3,
                                   observed=obs, fixed=fixed))

        for edge in self.edges:
            for g in edge.given:
                for p in edge.probability_of:
                    pgm.add_edge(node_name_dict[g], node_name_dict[p])
        pgm.render()
        if filename is not None:
            self.logger.debug("Saving figure to %s" % filename)
            pgm.figure.savefig(filename, transparent=True, dpi=300)

        return pgm

    def fit_model(self, num_walkers=None, num_steps=5000, num_burn=3000, temp_dir=None,
                  save_interval=300):
        """ Uses ``emcee`` to fit the supplied framework.

        This method sets an emcee run using the ``EnsembleSampler`` and manual
        chain management to allow for very high dimension models. MPI running
        is detected automatically for less hassle, and chain progress is serialised
        to disk automatically for convenience.

        This method works... but is still a work in progress

        Parameters
        ----------
        num_walkers : int, optional
            The number of walkers to run. If not supplied, it defaults to eight times the
            framework dimensionality
        num_steps : int, optional
            The number of steps to run
        num_burn : int, optional
            The number of steps to discard for burn in
        temp_dir : str
            If set, specifies a directory in which to save temporary results, like the emcee chain
        save_interval : float
            The amount of seconds between saving the chain to file. Setting to ``None``
            disables serialisation.

        Returns
        -------
        ndarray
            The final flattened chain of dimensions `
            `(num_dimensions, num_walkers * (num_steps - num_burn))``
        fig
            The corner plot figure returned from ``corner.corner(...)``
        """
        if not self._finalised:
            self.finalise()
        pool = None
        try:  # pragma: no cover
            pool = MPIPool()
            if not pool.is_master():
                self.logger.info("Slave waiting")
                self.master = False
                pool.wait()
                sys.exit(0)
            else:
                self.logger.info("MPIPool successful initialised and master found. "
                                 "Running with %d cores." % pool.size)
        except ImportError:
            self.logger.info("mpi4py is not installed or not configured properly. "
                             "Ignore if running through python, not mpirun")
        except ValueError as e:  # pragma: no cover
            self.logger.info("Unable to start MPI pool, expected normal python execution")
            self.logger.info(str(e))

        num_dim = len(self._theta_names)

        self.logger.debug("Fitting framework with %d dimensions"
                          % num_dim)
        if num_walkers is None:
            num_walkers = num_dim * 2
        num_walkers = max(num_walkers, 20)
        self.logger.info("Using Ensemble Sampler")
        sampler = emcee.EnsembleSampler(num_walkers, num_dim, self.get_log_posterior,
                                        pool=pool, live_dangerously=True)
        emcee_wrapper = EmceeWrapper(sampler)
        flat_chain = emcee_wrapper.run_chain(num_steps, num_burn, num_walkers, num_dim,
                                             start=self._get_starting_position,
                                             save_dim=self._num_actual,
                                             temp_dir=temp_dir,
                                             save_interval=save_interval)
        self.logger.debug("Fit finished")
        if pool is not None:  # pragma: no cover
            pool.close()
            self.logger.debug("Pool closed")
        self.flat_chain = flat_chain
        return flat_chain, \
               self._theta_names[:self._num_actual], \
               self._theta_labels[:self._num_actual]

    def get_consumer(self):
        from dessn.chain.chain import ChainConsumer
        chain_plotter = ChainConsumer()
        chain_plotter.add_chain(self.flat_chain,
                                parameters=self._theta_labels[:self._num_actual])
        return chain_plotter

    def __getstate__(self):  # pragma: no cover
        d = dict(self.__dict__)
        del d['logger']
        return d

    def __setstate__(self, d):  # pragma: no cover
        self.__dict__.update(d)
        if self.master:
            self.logger = logging.getLogger(__name__)


def _pickle_method(m):  # pragma: no cover
    if m.im_self is None:
        return getattr, (m.im_class, m.im_func.func_name)
    else:
        return getattr, (m.im_self, m.im_func.func_name)

copyreg.pickle(types.MethodType, _pickle_method)
