from dessn.model.node import Node, NodeObserved, NodeLatent, NodeUnderlying, NodeTransformation, NodeDiscrete
from dessn.model.edge import EdgeTransformation
from dessn.utility.hdemcee import EmceeWrapper
import numpy as np
import logging
import emcee
from emcee.utils import MPIPool
import sys
from scipy.optimize import fmin_bfgs
import types
import copy_reg
import itertools


class Model(object):
    """ A generalised model for use in arbitrary situations.

    A model is, at heart, simply a collection of nodes and edges. Apart from simply
    being a container in which to place nodes and edges, the model is also responsible for
    figuring out how to connect edges (which map to parameters) with the right nodes, for sorting edges
    such that when an edge is evaluated all its required data has been generated by other nodes or edges,
    for managing the ``emcee`` running, and also for generating the visual PGMs.

    It is thus a complex class, and I expect, as of writing this summary, it contains numerous bugs.

    Parameters
    ----------
    model_name : str
        The model name, used for serialisation
    """

    def __init__(self, model_name):
        self.model_name = model_name
        self.logger = logging.getLogger(__name__)
        self.nodes = []
        self.edges = []
        self._node_dict = {}
        self._node_indexes = {}
        self._observed_nodes = []
        self._latent_nodes = []
        self._transformation_nodes = []
        self._underlying_nodes = []
        self._discrete_nodes = []
        self._discrete_params = []
        self._in = {}
        self._out = {}
        self._theta_names = []
        self._theta_labels = []
        self._ordered_edges = []
        self._num_actual = None
        self.data = []
        self._finalised = False
        self.flat_chain = None
        self.num_temps = None
        self._labels = []
        self.master = True
        self._node_groups = {}

    def add_node(self, node):
        """ Adds a node into the models collection of nodes.

        Parameter
        ---------
        node : :class:`.Node`
        """
        assert isinstance(node, Node), "Supplied parameter is not a recognised Node object"
        assert node.name not in self._node_dict.keys(), "Parameter %s is already in the model" % node.name
        assert node.label not in self._labels, "Label %s is already in the model" % node.label
        self._labels.append(node.label)
        self.nodes.append(node)
        self._node_dict[node.name] = node
        self._in[node.name] = []
        self._out[node.name] = []
        if isinstance(node, NodeObserved):
            self._observed_nodes.append(node)
        elif isinstance(node, NodeDiscrete):
            self._discrete_nodes.append(node)
            self._discrete_params.append(node.name)
        elif isinstance(node, NodeLatent):
            self._latent_nodes.append(node)
        elif isinstance(node, NodeTransformation):
            self._transformation_nodes.append(node)
        elif isinstance(node, NodeUnderlying):
            self._underlying_nodes.append(node)

        self._finalised = False

    def add_edge(self, edge):
        """ Adds an edge into the models collection of edges

        Parameter
        ---------
        edge : :class:`.Edge`
        """
        self.edges.append(edge)
        for p in edge.probability_of:
            for g in edge.given:
                self._in[g].append(p)
                self._out[p].append(g)
        self._finalised = False

    def _validate_model(self):
        assert len(self._underlying_nodes) > 0, "No underlying model to constrain"
        assert len(self._observed_nodes) > 0, "No observed nodes found"
        for node in self.nodes:
            name = node.name
            if isinstance(node, NodeObserved):
                pass
                # assert len(self._in[name]) == 0, "Observed parameter %s should not have incoming edges" % name
                # assert len(self._out[name]) > 0, "Observed parameter %s is not utilised in the PGM" % name
            elif isinstance(node, NodeLatent) or isinstance(node, NodeTransformation):
                pass
                # assert len(self._in[name]) > 0, "Internal parameter %s has no incoming edges" % name
                # assert len(self._out[name]) > 0, "Internal parameter %s does not have any outgoing edges" % name
            elif isinstance(node, NodeUnderlying):
                assert len(self._in[name]) > 0, "Underlying parameter %s has no incoming edges" % name
                assert len(self._out[name]) == 0, "Underlying parameter %s should not have an outgoing edge" % name

    def _create_data_structures(self):
        names = [node.name for node in self._observed_nodes]
        datas = [node.data for node in self._observed_nodes]
        for d in zip(*datas):
            self.data.append(dict((name, datapoint)for name, datapoint in zip(names, d)))
        print(self.data)

        for node in self._underlying_nodes:
            self._theta_names.append(node.name)
            self._theta_labels.append(node.label)
        self._num_actual = len(self._theta_names)
        for node in self._latent_nodes:
            for name in node.names:
                self._theta_names += [name] * node.get_num_latent()

        num_edges = len(self.edges)
        observed_names = [node.name for node in self.nodes if not isinstance(node, NodeTransformation)]
        self._ordered_edges = []
        count = 0
        max_count = 100
        while len(self._ordered_edges) < num_edges:
            for edge in self.edges:
                if edge in self._ordered_edges:
                    continue
                if isinstance(edge, EdgeTransformation):
                    requirements = edge.given
                else:
                    requirements = edge.given + edge.probability_of
                unsatisfied_requirements = [r for r in requirements if r not in observed_names]
                print(edge, unsatisfied_requirements, edge.given, edge.probability_of)
                if len(unsatisfied_requirements) == 0:
                    self._ordered_edges.append(edge)
                    if isinstance(edge, EdgeTransformation):
                        for val in edge.probability_of:
                            observed_names.append(val)
            count += 1
            if count > max_count:
                raise ValueError("Model edges cannot be ordered. Please double check your edges")
        print(self._ordered_edges)

    def finalise(self):
        """ Finalises the model.

        This method runs consistency checks on the model (making sure there are not orphaned
        nodes, edges to parameters that do not exist, etc), and in doing so links the right
        edges to the right nodes and determines the order in which edges should be evaluated.

        You can manually call this method after setting all nodes and edges to confirm as early
        as possible that the model is valid. If you do not call it manually, this method
        is invoked by the model when requesting concrete information, such as the PGM or model fits.
        """
        self._validate_model()
        self._create_data_structures()
        self._finalised = True
        self.logger.info("Model validation passed")

    def _get_theta_dicts(self, theta):
        result = self.data.copy()
        make_array = []
        for theta, theta_name in zip(theta, self._theta_names):
            if self._theta_names.count(theta_name) == 1:
                result[theta_name] = theta
            else:
                if theta_name not in result:
                    result[theta_name] = []
                    make_array.append(theta_name)
                result[theta_name].append(theta)
        for m in make_array:
            result[m] = np.array(result[m])
        return result

    def get_log_prior(self, theta):
        theta_dict = self._get_theta_dict(theta)
        return self._get_log_prior(theta_dict)

    def get_log_likelihood(self, theta):
        theta_dict = self._get_theta_dict(theta)
        probability = 0
        for edge in self._ordered_edges:
            if isinstance(edge, EdgeTransformation):
                theta_dict.update(self._get_transformation(theta_dict, edge))
            else:
                result = self._get_log_likelihood(theta_dict, edge)
                probability += result
        return probability

    def _expand_discrete(self, dictionary):
        keys = dictionary.keys()
        product = list(itertools.product(*[dictionary[k] for k in keys]))
        return keys, product

    def _get_dependencies(self, edges, dependency_name):
        dep_edge = []
        for edge in edges:
            if isinstance(edge, EdgeTransformation) and dependency_name in edge.given:
                for name in edge.probability_of:
                    dep_edge += self._get_dependencies(edges, name)
            elif dependency_name in edge.given + edge.probability_of:
                dep_edge.append(edge)
        return dep_edge

    def _get_edge_likelihood(self, theta_dict, edges):
        probability = 0.0
        for edge in edges:
            dependencies = edge.given + edge.probability_of
            discretes = [parameter for parameter in dependencies if isinstance(self._node_dict[parameter], NodeDiscrete)]
            unfilled = [d for d in discretes if d not in theta_dict]
            unfilled_nodes = [self._node_dict[n] for n in unfilled]
            print(unfilled)
            if len(unfilled) > 0:
                first_name = unfilled[0]
                first_node = self._node_dict[first_name]
                print("first", first_name)
                unfilled_dependencies = first_node.get_discrete_requirements()
                print("unfilled dependencies", unfilled_dependencies)
                to_pass = dict((k, theta_dict[k]) for k in unfilled_dependencies)
                discrete = first_node.get_discrete(to_pass)
                print("discrete", discrete)
                for d in discrete:
                    theta_dict.update({first_name: d})
                    dependent_edges = self._get_dependencies(edges, first_name)
                    edges = [e for e in edges if e not in dependent_edges]
                    probability += self._get_edge_likelihood(theta_dict, dependent_edges)

            else:
                if isinstance(edge, EdgeTransformation):
                    theta_dict.update(self._get_transformation(theta_dict, edge))
                else:
                    probability += self._get_log_likelihood(theta_dict, edge)
        return probability

    def _get_log_posterior(self, theta):
        theta_dicts = self._get_theta_dict(theta)
        probability = self._get_log_prior(theta_dict)
        if np.isfinite(probability):
            probability += self._get_edge_likelihood(theta_dict, self._ordered_edges[:])
        return probability

    def _get_negative_log_posterior(self, theta):
        val = self._get_log_posterior(theta)
        # print(val)#, theta)
        return -val

    def _get_suggestion(self):
        node_sorted = []
        for name in self._theta_names:
            node = self._node_dict[name]
            if node not in node_sorted:
                node_sorted.append(node)

        theta = []
        data = self.data
        for node in node_sorted:
            node_data = dict((key, data[key]) for key in data if key in node.get_suggestion_requirements())
            temp_arr = node.get_suggestion(node_data)
            theta += temp_arr

        return theta

    def _get_starting_position(self, num_walkers):
        num_dim = len(self._theta_names)
        self.logger.debug("Generating starting guesses")
        p0 = self._get_suggestion()
        self.logger.debug("Initial position is:  %s" % p0)
        if False and self.num_temps is None:
            optimised = fmin_bfgs(self._get_negative_log_posterior, p0)
            self.logger.debug("Starting position is: %s" % optimised)
        else:
            optimised = p0

        if self.num_temps is not None:
            std = np.random.uniform(0.7, 1.3, size=(self.num_temps, num_walkers, num_dim))
        else:
            std = np.random.uniform(0.7, 1.3, size=(num_walkers, num_dim))
        start = std * optimised
        return start

    def _get_log_prior(self, theta_dict):
        result = []
        for node in self._underlying_nodes:
            p = node.get_log_prior(dict((key, theta_dict[key]) for key in node.names))
            if np.isnan(p):
                self.logger.error("Got NaN probability from %s: %s" % (node, theta_dict))
                raise ValueError("NaN")

            result.append(p)
        return np.sum(result)

    def _get_transformation(self, theta_dict, edge):
        return edge.get_transformation(dict((key, theta_dict[key]) for key in edge.given))

    def _get_log_likelihood(self, theta_dict, edge):
        result = edge.get_log_likelihood(dict((key, theta_dict[key]) for key in edge.given + edge.probability_of))
        if np.isnan(result):
            self.logger.error("Got NaN probability from %s: %s" % (edge, theta_dict))
            raise ValueError("NaN")
        return result

    def get_pgm(self, filename=None):
        """ Renders (and returns) a PGM of the current model.

        Parameters
        ----------
        filename : str, optional
            if the filename is set, the PGM is saved to file in the top level ``plots`` directory.

        Returns
        -------
        :class:`daft.PGM`
            The ``daft`` PGM class, for further customisation if required.
        """
        from dessn.utility.newtonian import NewtonianPosition
        from matplotlib import rc
        import daft
        if not self._finalised:
            self.finalise()

        self.logger.info("Generating PGM")
        rc("font", family="serif", size=8)
        rc("text", usetex=True)

        x_size = 9
        y_size = 8
        border = 1
        node_name_dict = {}

        n = []
        e = []
        t = []
        b = []

        for node in self.nodes:
            n.append(node.name)
            if node in self._observed_nodes:
                b.append(node.name)
            if node in self._underlying_nodes:
                t.append(node.name)
            for name in node.names:
                node_name_dict[name] = node.node_name

        for edge in self.edges:
            for g in edge.given:
                for p in edge.probability_of:
                    e.append([node_name_dict[g], node_name_dict[p]])

        self.logger.debug("Using Newtonian positioner to position %d nodes and %d edges" % (len(n), len(e)))

        positioner = NewtonianPosition(n, e, top=t, bottom=b)
        x, y = positioner.fit()
        x = (x_size - 2 * border) * x + border
        y = (y_size - 2 * border) * y + border

        self.logger.debug("Creating PGM from positioner results")
        pgm = daft.PGM([x_size, y_size], origin=[0., 0.2], observed_style='inner')
        for node, x, y in zip(self.nodes, x, y):
            obs = node in self._observed_nodes
            fixed = node in self._transformation_nodes
            node_name = node.node_name
            node_label = node_name.replace(" ", "\n") + "\n" + ", ".join(node.labels)

            pgm.add_node(daft.Node(node_name, node_label, x, y, scale=1.6, aspect=1.3, observed=obs, fixed=fixed))

        for edge in self.edges:
            for g in edge.given:
                for p in edge.probability_of:
                    pgm.add_edge(node_name_dict[g], node_name_dict[p])
        pgm.render()
        if filename is not None:
            self.logger.debug("Saving figure to %s" % filename)
            pgm.figure.savefig(filename, transparent=True, dpi=300)

        return pgm

    def fit_model(self, num_temps=None, num_walkers=None, num_steps=5000, num_burn=3000, temp_dir=None, save_interval=300):
        """ Uses ``emcee`` to fit the supplied model.

        This method sets an emcee run using the ``EnsembleSampler`` and manual chain management to allow for
        very high dimension models. MPI running is detected automatically for less hassle, and chain progress is
        serialised to disk automatically for convenience.

        This method works... but is still a work in progress

        Parameters
        ----------
        num_temps : int, optional
            The number of temperature to run. If none, does not use PTSampler
        num_walkers : int, optional
            The number of walkers to run. If not supplied, it defaults to eight times the model dimensionality
        num_steps : int, optional
            The number of steps to run
        num_burn : int, optional
            The number of steps to discard for burn in
        temp_dir : str
            If set, specifies a directory in which to save temporary results, like the emcee chain
        save_interval : float
            The amount of seconds between saving the chain to file. Setting to ``None`` disables serialisation.

        Returns
        -------
        ndarray
            The final flattened chain of dimensions ``(num_dimensions, num_walkers * (num_steps - num_burn))``
        fig
            The corner plot figure returned from ``corner.corner(...)``
        """
        if not self._finalised:
            self.finalise()
        pool = None
        try:
            pool = MPIPool()
            if not pool.is_master():
                self.logger.info("Slave waiting")
                self.master = False
                pool.wait()
                sys.exit(0)
            else:
                self.logger.info("MPIPool successful initialised and master found. Running with %d cores." % pool.size)
        except ImportError:
            self.logger.info("mpi4py is not installed or not configured properly. Ignore if running through python, not mpirun")
        except ValueError as e:
            self.logger.info("Unable to start MPI pool, expected normal python execution")
            self.logger.info(str(e))

        num_dim = len(self._theta_names)

        self.num_temps = num_temps
        self.logger.debug("Fitting model with %d dimensions using %d temperature levels" % (num_dim, 0 if self.num_temps is None else self.num_temps))
        if num_walkers is None:
            if num_temps is None:
                num_walkers = num_dim * 8
            else:
                num_walkers = num_dim * 4

        self.logger.debug("Running emcee")
        if num_temps is None:
            self.logger.info("Using Ensemble Sampler")
            sampler = emcee.EnsembleSampler(num_walkers, num_dim, self._get_log_posterior, pool=pool, live_dangerously=True)
        else:
            self.logger.info("Using PTSampler")
            sampler = emcee.PTSampler(self.num_temps, num_walkers, num_dim, self.get_log_likelihood, self.get_log_prior, pool=pool)
        emcee_wrapper = EmceeWrapper(sampler)
        flat_chain = emcee_wrapper.run_chain(num_steps, num_burn, num_walkers, num_dim, start=self._get_starting_position, save_dim=self._num_actual, temp_dir=temp_dir, save_interval=save_interval)
        self.logger.debug("Fit finished")
        if pool is not None:
            pool.close()
            self.logger.debug("Pool closed")
        self.flat_chain = flat_chain
        return flat_chain, self._theta_names[:self._num_actual], self._theta_labels[:self._num_actual]

    def get_consumer(self):
        from dessn.chain.chain import ChainConsumer
        chain_plotter = ChainConsumer()
        chain_plotter.add_chain(self.flat_chain, parameters=self._theta_labels[:self._num_actual])
        return chain_plotter

    def __getstate__(self):
        d = dict(self.__dict__)
        del d['logger']
        return d

    def __setstate__(self, d):
        self.__dict__.update(d)
        if self.master:
            self.logger = logging.getLogger(__name__)


def _pickle_method(m):
    if m.im_self is None:
        return getattr, (m.im_class, m.im_func.func_name)
    else:
        return getattr, (m.im_self, m.im_func.func_name)

copy_reg.pickle(types.MethodType, _pickle_method)
